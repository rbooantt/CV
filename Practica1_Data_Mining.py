# -*- coding: utf-8 -*-
"""Practica_Grupal_MD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wy-_amu1rESrtx52wsU0siX4MzmcBwna

# Mineria de Datos - Grado en Estadistica y Empresa
## Practica Final - Andrés Rubio Lafuente

## Paquetes y Funciones

Cargamos los paquetes necesarios y definimos las funciones necesarias para realizar la practica.
"""

# Paquetes para el webscraping
import bs4
from bs4 import BeautifulSoup
import requests
import csv

# Paquetes basicos
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Paquetes para el arbol de regresion
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score

# Paquetes para el clustering
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from scipy.stats import t

# Creamos una funcion para realizar el web scraping
# La funcion accede a los atributos precio, nombre de la casa, superficie, numero de habitaciones y baños y planta.
def webscraping(url, inicio, final):
  caracteristicas = []
  for num_pag in range(inicio, final + 1):
       sig_pag = url + str(num_pag)
       respuesta = requests.get(str(sig_pag))
       soup = BeautifulSoup(respuesta.content, "html.parser")
       ad_secciones = soup.find_all('div', {'class': 'ad-preview__bottom'})

       for ad_seccion in ad_secciones:
            info_seccion = ad_seccion.find('div', {'class': 'ad-preview__info'})
            precio  = info_seccion.find('span', {'class': 'ad-preview__price'})
            nombre = info_seccion.find('a', {'class': 'ad-preview__title'})
            direccion = info_seccion.find('p', {'class': 'p-sm'})
            car_seccion = info_seccion.find_all('div', {'class': 'ad-preview__section'})[2]
            car_elementos = car_seccion.find_all('p', {'class': 'ad-preview__char p-sm'})
            car_dicc = {key: 'N/A' for key in ['Habitaciones', 'Baños', 'Area', 'Planta']}

            for car in car_elementos:
               car_texto = car.text.strip()
               if 'hab' in car_texto:
                 car_dicc['Habitaciones'] = car_texto
               elif 'baño' in car_texto:
                 car_dicc['Baños'] = car_texto
               elif 'm²' in car_texto:
                 car_dicc['Area'] = car_texto
               elif 'planta' in car_texto or 'bajo' in car_texto.lower():
                 car_dicc['Planta'] = car_texto

            datos = {'Nombre': nombre.text.strip(),
                     'Barrio': direccion.text.strip(),
                      **car_dicc,
                     'Precio': precio.text.strip()}
            caracteristicas.append(datos)

  df = pd.DataFrame(caracteristicas)
  return(df)

# Creamos una funcion para dibujar el dendrograma
def dendrograma(model, **kwargs):
    plt.figure(figsize = (20,7))
    cont = np.zeros(model.children_.shape[0])
    n = len(model.labels_)
    for i, j in enumerate(model.children_):
        cont1 = 0
        for idx in j:
            if idx < n:
                cont += 1
            else:
                cont1 += cont[idx - n]
        cont[i] = cont1
    linkage_matrix = np.column_stack([model.children_, model.distances_, cont]).astype(float)
    _ = dendrogram(linkage_matrix, **kwargs)

# Creamos una funcion para la silueta media
def silueta():
  silueta_media = []
  rango = range(2, 11)
  for k in rango:
    cluster = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward', n_clusters = k)
    labels = cluster.fit_predict(X)
    s_media = silhouette_score(X, labels)
    silueta_media.append(s_media)

  plt.figure(figsize = (12,5))
  plt.plot(rango, silueta_media, marker = 'o')
  plt.xlabel("K")
  plt.ylabel("Silueta media")
  plt.title("Silueta media para numero de clusters optimo")
  plt.show()

"""## Web Scraping

Seleccionamos la pagina de la que queremos extraer informacion y solicitamos extraer la informacion.
"""

pisos_url = 'https://www.pisos.com/venta/pisos-madrid_capital_zona_urbana/fecharecientedesde-desc/'
pisos_response = requests.get(pisos_url)
pisos_response

"""La respuesta a la solicitud es 200, lo que nos indica que nuestra solicitud ha sido aceptada y podemos realizar web scraping."""

pisos = BeautifulSoup(pisos_response.text, 'html.parser')

"""Llamamos a la funcion y buscamos las primeras 30 paginas."""

pisos_info = webscraping(pisos_url, 0, 29)
pisos_info[:3]

"""Carga y limpieza de datos:
1. Extraemos el distrito donde esta la casa que sera nuestra columna barrio.
2. Eliminamos la columna nombre de la casa, no nos proporciona ninguna informacion adicional pues ya tenemos el barrio.
3. Cambiamos la columna habitaciones, baños, area y planta para que solo se quede con el dato numerico. En el caso de los NA o valores distintos a numericos en el caso de la Planta, por ejemplo la planta Bajo, ponemos 0.
4. Eliminamos el caracter euro de la columna precio.  


"""

datos = pisos_info
datos['Barrio'] = datos['Barrio'].str.extract(r'Distrito (\w+(?:\s\w+)*)')
codigo_barrio = pd.factorize(datos['Barrio'])[0]
info_barrio = pd.DataFrame({'Barrio': datos['Barrio'], 'CODIGO_BARRIO': codigo_barrio})
info_barrio = info_barrio.drop_duplicates(subset = ['Barrio'])
print(info_barrio)

datos = datos.drop(['Nombre'], axis = 1)
datos['Barrio'] = pd.factorize(datos['Barrio'])[0]
datos['Precio'] = datos['Precio'].replace('A consultar', '0')
datos['Precio'] = datos['Precio'].str.replace('€', '').str.replace('.','').astype(int)
datos['Habitaciones'] = datos['Habitaciones'].replace('N/A', '0')
datos['Habitaciones'] = datos['Habitaciones'].str.replace('hab.', '').str.replace('.','').astype(int)
datos['Baños'] = datos['Baños'].str.extract('(\d+)')
datos['Area'] = datos['Area'].str.replace('m.','')
datos['Planta'] = datos['Planta'].str.extract('(\d+)')
datos = datos.fillna(0)
datos = datos.drop_duplicates()
datos[:7]

"""Guardamos los datos en un csv."""

from google.colab import drive
drive.mount('/content/drive')

datos.to_csv('/content/drive/MyDrive/Mineria de Datos/Practica Grupal/datos_casas.csv', index = False)

"""## Limpieza de Datos

Importamos los datos con los que vamos a trabajar.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Aqui tenemos la lista de barrios asi como el codigo que se ha asignado en el web scraping."""

codigo_barrio = list(range(21))
nombre_barrio = ['San Blas','Carabanchel','Centro','Salamanca',
                 'Hortaleza', 'Fuencarral', 'Moncloa', 'Tetuán',
                 'Puente de Vallecas', 'Retiro', 'Latina', 'Villa de Vallecas',
                 'Ciudad Lineal', 'Chamberí', 'Moratalaz', 'Usera', 'Villaverde',
                 'Chamartín', 'Arganzuela', 'Vicálvaro', 'Barajas']
info_barrio = pd.DataFrame({'Codigo': codigo_barrio, 'Nombre': nombre_barrio})
print(info_barrio)

"""Como vemos los codigos de los barrios no tienen ningun orden especifico, con lo cual es como si fuese una columna identificadora que no proporciona informacion ninguna. Ya veremos que hacemos con esta columna.

Cargamos los datos, que los habiamos guardado en un archivo csv tras finalizar el web scraping.
"""

datos = pd.read_csv('/content/drive/MyDrive/Mineria de Datos/Practica Grupal/datos_casas.csv')
datos[:3]

"""Antes de nada, vamos a ordenar los barrios para que sean de mas utilidad que una columna identificadora.

Tenemos 21 barrios, ordenados sin un orden especifico del 0 al 21.

Pues bien, nosotros los queremos ordenar de forma que el barrio 0 sea el mas caro y el barrio 21 sea el mas barato.

Para ello, agrupamos las casa por barrio, calculamos el precio medio por vivienda y ordenamos los barrios de mas caro a mas barato.
"""

precio_barrio = datos.groupby(['Barrio'])['Precio'].mean().astype(int)
i_barrio = np.argsort(precio_barrio)[::-1]
ord_barrio = precio_barrio.index[i_barrio]
media_barrio = pd.DataFrame({'Barrio': ord_barrio, 'Precio': precio_barrio.values[i_barrio]})
media_barrio[:5]

"""Creamos un diccionario que nos diga la nueva asignacion de codigos para cada barrio."""

dic_barrio = {barrio: codigo for codigo, barrio in enumerate(ord_barrio)}
dic_barrio

"""El nuevo orden de los barrios de mas caro a mas barato es el siguiente."""

info_barrio['Nuevo_Codigo'] = info_barrio['Codigo'].map(dic_barrio)
info_barrio.drop('Codigo', axis = 1, inplace = True)
info_barrio = info_barrio.sort_values(by = 'Nuevo_Codigo', ascending = True)
print(info_barrio)

"""Aplicamos este diccionario para transformar los datos."""

datos['Barrio'] = datos['Barrio'].map(dic_barrio)
datos[:3]

"""Una vez limpiados los datos, podemos pasar a realizar un EDA basico de los datos.

## EDA

Vemos las dimensiones de los datos con los que estamos trabajando.
"""

datos.shape

"""Tenemos 863 instancias, en este caso, casas, sobre las que se han medido 6 caracteristicas.

Vemos el tipo de datos con los que estamos trabajando.
"""

datos.dtypes

"""Tenemos que todas las variables son numericas.

Hacemos un resumen basico de los datos.
"""

datos.describe()

"""Vemos que los valores de las variables varian bastante en escala de columna a columna, luego sera necesario escalar los datos

Vemos si hay valores faltantes.
"""

suma_na = datos.isna().sum()
porcentaje_na = 100 * (suma_na/datos.shape[0])
df_na = pd.DataFrame({'Numero NA':suma_na, 'Porcentaje NA': porcentaje_na})
df_na

"""Eliminamos dichas instancias en las que no viene la superficie de la casa."""

datos = datos.dropna(subset = ['Area'])

"""Comprobamos que se han eliminado los valores faltantes."""

suma_na = datos.isna().sum()
porcentaje_na = 100 * (suma_na/datos.shape[0])
df_na = pd.DataFrame({'Numero NA':suma_na, 'Porcentaje NA': porcentaje_na})
df_na

"""Realizamos un histograma de todas las caracteristicas de las casas."""

datos.hist(figsize = (15, 10))
plt.show()

"""Realizamos un boxplot de la variable respuesta Precio."""

datos.boxplot(['Precio'])
plt.show()

"""Como vemos hay algunos valores atipicos, pero si aumentamos el rango intercuartilico practicamente no tenemos valores atipicos.

Obtenemos una matriz de correlaciones para ver las relaciones entre las variables.
"""

mcorr = datos.corr()
sns.heatmap(mcorr, annot = True)
plt.show()

"""Como vemos la mayoria de variables tienen una correlacion baja; sin embargo, hay algunas variables que estan bastante correladas, como son:

1. El numero de habitaciones con el numero de baños.
2. El numero de habitaciones con el area de la casa.
3. El numero de baños con el area de la casa.
4. Todas las variabales a excepcion de la planta estan relacionadas con el precio de la casa. Segun esto, el precio de la casa aumenta si aumenta el numero de baños, de habitaciones, el area y si disminuye el barrio, nos vamos a barrios mas caros.

A pesar de todas estas relaciones, no encontramos 2 variables que esten extremadamente correladas, luego no tenemos que considerar la eliminacion de ninguna variable. Como mucho, podriamos considerar eliminar la variable 'Planta', pues no parece que este muy relacionada con el resto de variables, pero nosotros hemos decidido mantenerla.

Vemos ahora si hay variables con atributos constantes.
"""

datos.columns[datos.nunique() == 1]

"""No encontramos ninguna variable con atributos constantes; es decir, todas las variables contienen informacion mas o menos util para el modelo.

Por ultimo, podemos ver los precios medios de las casas segun el barrio en el que estemos.
"""

plt.figure(figsize = (10,6))
plt.bar(info_barrio['Nuevo_Codigo'], media_barrio['Precio'], color = 'blue')
plt.xlabel('Barrio')
plt.ylabel('Precio')
plt.title('Precio medio por barrio')
plt.show()

"""## Modelo Arbol de Regresion

Dividimos los datos en las variables regresoras X y la variable respuesta Y.
"""

X = datos.drop(['Precio'], axis = 1)
y = datos['Precio']

"""Escalamos los datos"""

scaler = StandardScaler()
X_trans = scaler.fit_transform(X)

"""Dividimos los datos en entrenamiento y test utilizando un metodo holdout al 25%; es decir, dedicamos 3/4 de los datos a entrenamiento y un 1/4 a test."""

X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size = (1/4), random_state = 129)

"""Ajustamos los hiperparametros del modelo del arbol de regresion, que son la maxima profundidad del arbol y el numero minimo de instancias necesarias para dividir el arbol.

Como metrica de evaluacion del modelo, hemos decidido utilizar el R2, que nos indica el porcentaje de la variabilidad de la variable respuesta que podemos explicar con nuestro modelo.

Por tanto, nuestro objetivo sera maximizar el valor de R2, que esta comprendido entre 0 y 1, donde 1 es que explicamos perfectamente la variabilidad de la variable respuesta.  
"""

arbol = DecisionTreeRegressor(random_state = 129)
param_grid = {'max_depth': range(3,7),
              'min_samples_split': range(10,20)}
grid_search = GridSearchCV(arbol, param_grid, cv = 10, scoring = 'r2')

"""Entrenamos el modelo con los datos de entrenamiento."""

grid_search.fit(X_train, y_train)

"""Obtenemos los mejores valores de los hiperparametros."""

grid_search.best_params_

"""Obtenemos el mejor modelo con dichos hiperparametros."""

mejor_arbol = grid_search.best_estimator_

"""Obtenemos el score del modelo, que es el score de entrenamiento."""

r2_train = grid_search.best_score_
print(f'Entrenamiento r2 = {r2_train}')

"""Calculamos las predicciones con los datos de test y calculamos el valor R2 de los datos de test."""

y_test_pred = mejor_arbol.predict(X_test).astype(int)
r2_test = r2_score(y_test, y_test_pred)
print(f'Test r2 = {r2_test}')

"""Obtenemos un mejor valor de r2 para los datos de test que para los datos de entrenamiento, luego el modelo generaliza muy bien y no se sobreajusta a los datos de entrenamiento.

Obtenemos un R2 = 0.7488, luego con el modelo de arbol de regresion somos capaces de explicar aproximadamente un 75% de la variabilidad de la variable respuesta, lo cual es un resultado bastante decente.

Por ultimo, mostramos las predicciones de los datos de test frente a los valores reales.
"""

pred_test = pd.DataFrame({'Valor Real': y_test, 'Valor Predicho': y_test_pred})
pred_test[:5]

"""Observamos que entre los valores reales y los predichos hay una ligera diferencia que hace referencia a ese 25% restante de la variabilidad que no podemos explicar con el modelo.

## Modelo Clustering Dendrograma

Aunque ya sabemos como se agrupan las instancias de nuestro dataframe, que son casas y se agrupan por barrios, vamos a considerar una nueva agrupacion de las casas, pues tenemos demasiados barrios y queremos reducir el numero de grupos.

Para ello, creamos un modelo de clustering para agrupar las instancias por grupos lo mas homogeneo posible dentro de cada grupo y lo mas heterogeneo posible entre los grupos.  

Definimos el modelo de clustering en el que utilizamos la distancia euclidea y se utiliza el criterio de Ward que minimiza la varianza dentro de cada grupo.
"""

modelo_cluster = AgglomerativeClustering(n_clusters = None, affinity = 'euclidean', linkage = 'ward', distance_threshold = 0)
modelo_cluster.fit(X)

"""Hacemos un dendrograma del modelo de clustering que acabamos de definir."""

dendrograma(modelo_cluster, truncate_mode = 'level')
plt.title("Dendograma Casas")
plt.show()

"""Marcamos el punto de corte para el dendrograma."""

dendrograma(modelo_cluster, color_threshold = 1000)
plt.title("Dendrograma Casas")
plt.axhline(y = 1000, c = 'black', linestyle = '--')
plt.show()

"""Calculamos el mejor valor de la silueta media."""

silueta()

"""Buscamos obtener el numero de grupos k optimo de forma que se maximice el valor del coeficiente de la silueta media.

Como vemos, el valor k donde se maximiza la silueta media es k = 2, luego el modelo de clustering divide los datos en 2 grupos.

Para ver como se dividen los datos creamos un modelo de kmedias con 2 grupos o clusters.
"""

kmedias = KMeans(init = "random", random_state = 129, n_clusters = 2)
kmedias.fit(X)
clusters = kmedias.predict(X)

"""Vemos cuantas instancias pertenecen a cada grupo."""

datos['ID'] = kmedias.labels_
id = datos['ID']
num_grupos = id.nunique()
for grupo in range(num_grupos):
    numero_grupo = len(id[id == grupo])
    print(f"Número de instancias en el Grupo {grupo + 1}: {numero_grupo}")

"""Una vez creado el modelo, representamos las caracteristicas de los grupos que ha determinado el metodo kmedias.

Primero graficamos los valores de los precios que se han determinado para cada grupo de casas.
"""

datos['Cluster'] = clusters
X_precio = datos[['Precio','Cluster']]
X_precio_media = X_precio.groupby('Cluster').mean()
X_precio_media.plot(kind = 'bar', figsize = (10,6))
plt.show()

"""Ahora graficamos la superficie de las casas para los 2 grupos definidos."""

X_area = datos[['Area','Cluster']]
X_area_media = X_area.groupby('Cluster').mean()
X_area_media.plot(kind = 'bar', figsize = (10,6))
plt.show()

"""Por ultimo, graficamos el resto de caracteristicas de la casa."""

X_caracteristicas = datos.drop(['ID','Precio','Area'],axis = 1)
X_caracteristicas_media = X_caracteristicas.groupby('Cluster').mean()
X_caracteristicas_media.plot(kind = 'bar', figsize = (10,6))
plt.show()

"""Podemos ver todas las caracteristicas medias de los 2 grupos resumidas en la siguiente tabla."""

Xf = datos.drop(['ID'], axis = 1)
X_medias = Xf.groupby('Cluster').mean()
X_medias

"""Aunque, para el precio es de mayor informacion la mediana mas que la media pues hemos visto que hay valores atipicos que desvian la media de su verdadero valor."""

X_medianas = Xf.groupby('Cluster').median()
X_medianas

"""Tambien podemos ver intervalos de confianza para cada caracteristica de cada grupo.

Utilizamos intervalos de confianza basados en la distribucion normal, pues  el tamaño de cada grupo es n > 30.
"""

conf_level = 0.95
X_medias = Xf.groupby('Cluster').mean()
X_desv =  Xf.groupby('Cluster').std()

"""Intervalo de confianza (IC) para el primer grupo."""

n1 = 109
df1 = n1 - 1
se1 = X_desv.iloc[0] / np.sqrt(n1)
t_est1 = t.ppf((1 + conf_level) / 2, df1)
inf = X_medias.iloc[0] - t_est1 * se1
sup = X_medias.iloc[0] + t_est1 * se1
X_medias_f = X_medias.iloc[0].astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
inf_f = inf.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
sup_f = sup.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
ic = pd.concat([X_medias_f, inf_f, sup_f], axis = 1, keys = ['Media', 'IC Inferior', 'IC Superior'])
print(ic)

"""La media del barrio de las casas del primer grupo es del nivel economico de los barrios 4 o 5 que son Moncloa o Centro.

El numero medio de habitaciones es 4 y el numero medio de baños es 3.

El area media de las casas esta entre 245 y 280 metros cuadrados.

La planta media de las casas esta entre 1 y 2.

El precio medio de las casas esta entre 1.6 y 2 millones de euros, pero su mediana es 1.4 millones de euros.

Intervalo de confianza (IC) para el segundo grupo.
"""

n2 = 751
df2 = n2 - 1
se2 = X_desv.iloc[1] / np.sqrt(n2)
t_est2 = t.ppf((1 + conf_level) / 2, df2)
inf2 = X_medias.iloc[1] - t_est2 * se2
sup2 = X_medias.iloc[1] + t_est2 * se2
X_medias_f2 = X_medias.iloc[1].astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
inf_f2 = inf2.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
sup_f2 = sup2.astype(float).round(2).apply(lambda x: '{:0g}'.format(x))
ic2 = pd.concat([X_medias_f2, inf_f2, sup_f2], axis = 1, keys = ['Media', 'Inferior', 'Superior'])
print(ic2)

"""La media del barrio de las casas del primer grupo es del nivel economico de los barrios 9 o 10 que son Barajas o Ciudad Lineal.

El numero medio de habitaciones es 2 y el numero medio de baños es 1.5.

El area media de las casas esta entre 83 y 88 metros cuadrados.

La planta media de las casas esta entre 1.3 y 1.7.

El precio medio de las casas esta entre 387000 y 434000 de euros, pero su mediana es 288000 euros.
"""